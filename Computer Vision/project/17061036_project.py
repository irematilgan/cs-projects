# -*- coding: utf-8 -*-
"""cv_project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Ewd-Gh4Z6GBzAkfrAJRAag-sfZ4KKsWg
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, Dropout, Input
from keras.initializers import glorot_normal
from keras.callbacks import EarlyStopping, CSVLogger, LearningRateScheduler, ModelCheckpoint, ReduceLROnPlateau
from keras.models import Model,Input,Sequential,load_model
from keras.preprocessing.image import ImageDataGenerator
from keras.applications.vgg16 import VGG16
from keras.applications.resnet50 import ResNet50
from google.colab import files
import os
import cv2
from PIL import Image
import tensorflow as tf
from sklearn.metrics import confusion_matrix,classification_report
import seaborn as sns

# %matplotlib inline

!pip install -q kaggle

files.upload()

!mkdir ~/.kaggle
!cp kaggle.json ~/.kaggle
!chmod 600 ~/.kaggle/kaggle.json

!mkdir cinic-10/

!kaggle datasets download -d mengcius/cinic10 --path 'cinic-10/' --unzip

train_images = []
test_images = []
valid_images = []
train_tags = []
test_tags = []
valid_tags = []

def upload_images(data_type):
  img_list = []
  tag_list = []
  PATH = "/content/cinic-10/"+data_type+"/"
  cats = os.listdir(PATH)
  for cname in cats:
    f_cat = PATH + cname + "/"
    for img_name in os.listdir(f_cat)[:200]:
      f_img = f_cat + img_name
      img = cv2.imread(f_img)
      img_list.append(img)
      tag_list.append(cname)

  return img_list, tag_list

train_images, train_tags = upload_images('train')
test_images, test_tags = upload_images('test')
valid_images, valid_tags = upload_images('valid')

resized_train = [cv2.resize(image,(224,224),interpolation = cv2.INTER_LINEAR) for image in train_images]

resized_test = [cv2.resize(image,(224,224),interpolation = cv2.INTER_LINEAR) for image in test_images]

resized_valid = [cv2.resize(image,(224,224),interpolation = cv2.INTER_LINEAR) for image in valid_images]

input_shape = (224,224,3)
train_images = np.reshape(resized_train,(len(resized_train),input_shape[0],input_shape[1],input_shape[2]))
test_images = np.reshape(resized_test,(len(resized_test),input_shape[0],input_shape[1],input_shape[2]))
valid_images = np.reshape(resized_valid,(len(resized_valid),input_shape[0],input_shape[1],input_shape[2]))

train_images.shape

train_tags = np.reshape(train_tags,(len(train_tags),1))
test_tags = np.reshape(test_tags,(len(train_tags),1))
valid_tags = np.reshape(valid_tags,(len(train_tags),1))

train_tags.shape

train_random = np.random.permutation(len(train_tags))
x_train, y_train = train_images[train_random],train_tags[train_random]

x_train.shape

test_random = np.random.permutation(len(test_tags))
x_test, y_test = test_images[test_random],train_tags[test_random]
valid_random = np.random.permutation(len(valid_tags))
x_valid, y_valid = valid_images[valid_random],train_tags[valid_random]

"""# CREATE SEQUENTIAL MODEL

---


"""

def get_model(num_filters,filter_size, layer_size, initializer_func, dropout_ratio):
  inputs = Input(shape = (224,224,3))

  model = Sequential()
  model.add(inputs)
  model.add(Conv2D(num_filters,(filter_size,filter_size),kernel_initializer= initializer_func,activation = 'relu'))
  model.add(MaxPool2D((2,2)))
  model.add(Dropout(dropout_ratio))
  for layer in range(layer_size-1):
    model.add(Conv2D(num_filters,(filter_size,filter_size),kernel_initializer= initializer_func,activation = 'relu'))
    model.add(MaxPool2D((2,2)))
    model.add(Dropout(dropout_ratio))

  model.add(Flatten())
  model.add(Dense(filter_size))
  model.add(Dense(10,activation = 'softmax'))

  return model

inputs = Input(shape = (224,224,3))

#LAYER 1, DROPOUT 0.2

"""model_filter16_s3_rms_l1_d02 = get_model(16,3,1,glorot_normal(),0.2)
model_filter32_s3_rms_l1_d02 = get_model(32,3,1,glorot_normal(),0.2)
model_filter64_s3_rms_l1_d02 = get_model(64,3,1,glorot_normal(),0.2)

model_filter16_s5_rms_l1_d02 = get_model(16,5,1,glorot_normal(),0.2)
model_filter32_s5_rms_l1_d02 = get_model(32,5,1,glorot_normal(),0.2)
model_filter64_s5_rms_l1_d02 = get_model(64,5,1,glorot_normal(),0.2)

model_filter16_s3_adam_l1_d02 = get_model(16,3,1,glorot_normal(),0.2)
model_filter32_s3_adam_l1_d02 = get_model(32,3,1,glorot_normal(),0.2)
model_filter64_s3_adam_l1_d02 = get_model(64,3,1,glorot_normal(),0.2)

model_filter16_s5_adam_l1_d02 = get_model(16,5,1,glorot_normal(),0.2)
model_filter32_s5_adam_l1_d02 = get_model(32,5,1,glorot_normal(),0.2)
model_filter64_s5_adam_l1_d02 = get_model(64,5,1,glorot_normal(),0.2)"""

#LAYER 2, DROPOUT 0.2
"""model_filter16_s3_rms_l2_d02 = get_model(16,3,2,glorot_normal(),0.2)
model_filter32_s3_rms_l2_d02 = get_model(32,3,2,glorot_normal(),0.2)
model_filter64_s3_rms_l1_d02 = get_model(64,3,2,glorot_normal(),0.2)

model_filter16_s5_rms_l2_d02 = get_model(16,5,2,glorot_normal(),0.2)
model_filter32_s5_rms_l2_d02 = get_model(32,5,2,glorot_normal(),0.2)
model_filter64_s5_rms_l2_d02 = get_model(64,5,2,glorot_normal(),0.2)

model_filter16_s3_adam_l2_d02 = get_model(16,3,2,glorot_normal(),0.2)
model_filter32_s3_adam_l2_d02 = get_model(32,3,2,glorot_normal(),0.2)
model_filter64_s3_adam_l2_d02 = get_model(64,3,2,glorot_normal(),0.2)

model_filter16_s5_adam_l2_d02 = get_model(16,5,2,glorot_normal(),0.2)
model_filter32_s5_adam_l2_d02 = get_model(32,5,2,glorot_normal(),0.2)
model_filter64_s5_adam_l2_d02 = get_model(64,5,2,glorot_normal(),0.2)"""

#LAYER 3, DROPOUT 0.2
model_filter16_s3_rms_l3_d02 = get_model(16,3,3,glorot_normal(),0.2)
model_filter32_s3_rms_l3_d02 = get_model(32,3,3,glorot_normal(),0.2)
model_filter64_s3_rms_l3_d02 = get_model(64,3,3,glorot_normal(),0.2)

model_filter16_s5_rms_l3_d02 = get_model(16,5,3,glorot_normal(),0.2)
model_filter32_s5_rms_l3_d02 = get_model(32,5,3,glorot_normal(),0.2)
model_filter64_s5_rms_l3_d02 = get_model(64,5,3,glorot_normal(),0.2)

model_filter16_s3_adam_l3_d02 = get_model(16,3,3,glorot_normal(),0.2)
model_filter32_s3_adam_l3_d02 = get_model(32,3,3,glorot_normal(),0.2)
model_filter64_s3_adam_l3_d02 = get_model(64,3,3,glorot_normal(),0.2)

model_filter16_s5_adam_l3_d02 = get_model(16,5,3,glorot_normal(),0.2)
model_filter32_s5_adam_l3_d02 = get_model(32,5,3,glorot_normal(),0.2)
model_filter64_s5_adam_l3_d02 = get_model(64,5,3,glorot_normal(),0.2)

###################################################################

#LAYER 1, DROPOUT 0.7
"""model_filter16_s3_rms_l1_d07 = get_model(16,3,1,glorot_normal(),0.7)
model_filter32_s3_rms_l1_d07 = get_model(32,3,1,glorot_normal(),0.7)
model_filter64_s3_rms_l1_d07 = get_model(64,3,1,glorot_normal(),0.7)

model_filter16_s5_rms_l1_d07 = get_model(16,5,1,glorot_normal(),0.7)
model_filter32_s5_rms_l1_d07 = get_model(32,5,1,glorot_normal(),0.7)
model_filter64_s5_rms_l1_d07 = get_model(64,5,1,glorot_normal(),0.7)

model_filter16_s3_adam_l1_d07 = get_model(16,3,1,glorot_normal(),0.7)
model_filter32_s3_adam_l1_d07 = get_model(32,3,1,glorot_normal(),0.7)
model_filter64_s3_adam_l1_d07 = get_model(64,3,1,glorot_normal(),0.7)

model_filter16_s5_adam_l1_d07 = get_model(16,5,1,glorot_normal(),0.7)
model_filter32_s5_adam_l1_d07 = get_model(32,5,1,glorot_normal(),0.7)
model_filter64_s5_adam_l1_d07 = get_model(64,5,1,glorot_normal(),0.7)"""

#LAYER 2, DROPOUT 0.7
"""model_filter16_s3_rms_l2_d07 = get_model(16,3,2,glorot_normal(),0.7)
model_filter32_s3_rms_l2_d07 = get_model(32,3,2,glorot_normal(),0.7)
model_filter64_s3_rms_l2_d07 = get_model(64,3,2,glorot_normal(),0.7)

model_filter16_s5_rms_l2_d07 = get_model(16,5,2,glorot_normal(),0.7)
model_filter32_s5_rms_l2_d07 = get_model(32,5,2,glorot_normal(),0.7)
model_filter64_s5_rms_l2_d02 = get_model(64,5,2,glorot_normal(),0.7)

model_filter16_s3_adam_l2_d07 = get_model(16,3,2,glorot_normal(),0.7)
model_filter32_s3_adam_l2_d07 = get_model(32,3,2,glorot_normal(),0.7)
model_filter64_s3_adam_l2_d07 = get_model(64,3,2,glorot_normal(),0.7)

model_filter16_s5_adam_l2_d07 = get_model(16,5,2,glorot_normal(),0.7)
model_filter32_s5_adam_l2_d07 = get_model(32,5,2,glorot_normal(),0.7)
model_filter64_s5_adam_l2_d07 = get_model(64,5,2,glorot_normal(),0.7)"""

#LAYER 3, DROPOUT 0.7
model_filter16_s3_rms_l3_d07 = get_model(16,3,3,glorot_normal(),0.7)
model_filter32_s3_rms_l3_d07 = get_model(32,3,3,glorot_normal(),0.7)
model_filter64_s3_rms_l3_d07 = get_model(64,3,3,glorot_normal(),0.7)

model_filter16_s5_rms_l3_d07 = get_model(16,5,3,glorot_normal(),0.7)
model_filter32_s5_rms_l3_d07 = get_model(32,5,3,glorot_normal(),0.7)
model_filter64_s5_rms_l3_d07 = get_model(64,5,3,glorot_normal(),0.7)

model_filter16_s3_adam_l3_d07 = get_model(16,3,3,glorot_normal(),0.7)
model_filter32_s3_adam_l3_d07 = get_model(32,3,3,glorot_normal(),0.7)
model_filter64_s3_adam_l3_d07 = get_model(64,3,3,glorot_normal(),0.7)

model_filter16_s5_adam_l3_d07 = get_model(16,5,3,glorot_normal(),0.7)
model_filter32_s5_adam_l3_d07 = get_model(32,5,3,glorot_normal(),0.7)
model_filter64_s5_adam_l3_d07 = get_model(64,5,3,glorot_normal(),0.7)

model_filter16_s3_adam_l3_d02.compile(loss = "categorical_crossentropy",optimizer = "adam",metrics = ["accuracy"])
model_filter16_s3_adam_l3_d07.compile(loss = "categorical_crossentropy",optimizer = "adam",metrics = ["accuracy"])

model_filter16_s3_adam_l3_d02.summary()

model_filter32_s3_adam_l3_d02.compile(loss = "categorical_crossentropy",optimizer = "Adam",metrics = ["accuracy"])

x_train = np.array(x_train,dtype = np.float64)/255

from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder().fit(y_train)
y_transformed = ohe.transform(y_train).toarray()

x_train.shape

y_transformed.shape

y_valid = ohe.transform(y_valid).toarray()

x_valid = np.array(x_valid,dtype = np.float64)/255

history = model_filter32_s3_adam_l3_d02.fit(x = x_train, y = y_transformed, epochs = 10, batch_size = 16, validation_data = (x_valid,y_valid))

y_pred = model_filter32_s3_adam_l3_d07.predict(x_test)

predictions_labels = np.array([pred.argmax() for pred in y_pred]).reshape(-1,1)
test_labels = np.array([t_pred.argmax() for t_pred in y_test]).reshape(-1,1)

print(classification_report(test_labels,predictions_labels))

cm = confusion_matrix(test_labels,predictions_labels)
fig, ax = plt.subplots(figsize=(13,10)) 
sns.heatmap(cm,annot = True, square = True,ax = ax,fmt = "d")

model_filter16_s3_adam_l3_d02.save('model_filter16_s3_adam_l3_d02')

# Commented out IPython magic to ensure Python compatibility.
# %config InlineBackend.figure_format = 'svg'
def plot_metric(history, metric):
    train_metrics = history.history[metric]
    val_metrics = history.history['val_'+metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.plot(epochs, val_metrics, 'ro-')
    plt.title('Training and validation '+ metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_"+metric, 'val_'+metric])
    plt.show()

plot_metric(history,'accuracy')

plot_metric(history,'loss')

x_test = np.array(x_test,dtype = np.float64)/255
y_test = ohe.transform(y_test).toarray()

model_filter16_s3_adam_l3_d02.evaluate(x_test)

"""## TRANSFER LEARNING

"""

model_res = ResNet50(include_top= False,weights = "imagenet",input_shape=(224,224,3))
output = model_res.output
model_res1 = Flatten()(output)
model_res1 = Dense(1024,activation = "relu")(model_res1)
model_res1 = Dense(10,activation = "softmax")(model_res1)
new_model = Model(model_res.input,model_res1)

for layer in new_model.layers[:-4]:
  layer.trainable = False

new_model.compile(loss = "categorical_crossentropy",optimizer = "adam",metrics = ["accuracy"])

y_valid.shape

history_transfer = new_model.fit(x_train,y_transformed,batch_size = 16, epochs = 10, validation_data=(x_valid,y_valid))

new_model.save('resnet50_adam')

plot_metric(history_transfer,'accuracy')

plot_metric(history_transfer,'loss')

new_model.evaluate(x_test,y_test)

model_res = VGG16(include_top= False,weights = "imagenet",input_shape=(224,224,3))
output = model_res.output
model_res1 = Flatten()(output)
model_res1 = Dense(1024,activation = "relu")(model_res1)
model_res1 = Dense(10,activation = "softmax")(model_res1)
new_model = Model(model_res.input,model_res1)

for layer in new_model.layers[:-4]:
  layer.trainable = False

new_model.compile(loss = "categorical_crossentropy",optimizer = "adam",metrics = ["accuracy"])

history_transfer = new_model.fit(x_train,y_transformed,batch_size = 16, epochs = 10, validation_data=(x_valid,y_valid))

